{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cb738f6-86b9-4e9a-9bc6-ace524acf147",
   "metadata": {
    "tags": []
   },
   "source": [
    "# CIDC_for_Machine_Learning_v1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afed017-3591-4f27-ad75-b3a9672110f6",
   "metadata": {},
   "source": [
    "ML_CICD_Pipeline\n",
    "\n",
    "\n",
    "https://github.com/Sobhan-Mohammadi/ML_CICD_Pipeline.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830ea0c4-8ae1-4880-abbd-0e22c49caaa5",
   "metadata": {},
   "source": [
    "Certainly! Below is a comprehensive guide from start to finish on setting up a full CI/CD pipeline for a machine learning project using GitHub Actions, Docker, and DVC. I will cover every aspect including project structure, necessary files, code snippets, and detailed instructions on how to interact with GitHub and set up the pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "## **Project Overview**\n",
    "\n",
    "This project is designed to predict house prices using a machine learning model. We will create a CI/CD pipeline that automates:\n",
    "- Data fetching and preprocessing\n",
    "- Model training\n",
    "- Model evaluation\n",
    "- Model deployment\n",
    "- Continuous integration with automated testing\n",
    "- Continuous deployment using Docker and GitHub Actions\n",
    "\n",
    "## **Project Structure**\n",
    "\n",
    "Here’s the directory structure for the project:\n",
    "\n",
    "```\n",
    "HousePricePrediction/\n",
    "├── data/\n",
    "│   ├── raw/                    # Raw data files\n",
    "│   └── processed/              # Processed data files\n",
    "├── models/                     # Trained models\n",
    "├── notebooks/                  # Jupyter notebooks (optional)\n",
    "├── scripts/                    # Python scripts\n",
    "│   ├── data_preprocessing.py   # Preprocesses the data\n",
    "│   ├── train_model.py          # Trains the model\n",
    "│   ├── evaluate_model.py       # Evaluates the model\n",
    "│   └── deploy_model.py         # Deploys the model (Flask app)\n",
    "├── tests/                      # Unit tests\n",
    "│   ├── test_data_preprocessing.py\n",
    "│   ├── test_train_model.py\n",
    "│   └── test_evaluate_model.py\n",
    "├── .github/\n",
    "│   └── workflows/\n",
    "│       └── ci_cd_pipeline.yml  # GitHub Actions configuration\n",
    "├── Dockerfile                  # Docker configuration\n",
    "├── Makefile                    # Makefile for common commands\n",
    "├── README.md                   # Project description\n",
    "├── dvc.yaml                    # DVC pipeline configuration\n",
    "├── params.yaml                 # Parameters and configuration\n",
    "├── requirements.txt            # Python dependencies\n",
    "└── setup.py                    # Python package configuration\n",
    "```\n",
    "\n",
    "## **Step-by-Step Guide**\n",
    "\n",
    "### **Step 1: Set Up the Project Locally**\n",
    "\n",
    "1. **Create and Activate Virtual Environment:**\n",
    "    ```bash\n",
    "    python3 -m venv venv\n",
    "    source venv/bin/activate\n",
    "    ```\n",
    "\n",
    "2. **Install Dependencies:**\n",
    "    ```bash\n",
    "    pip install -r requirements.txt\n",
    "    ```\n",
    "\n",
    "3. **Initialize Git Repository:**\n",
    "    ```bash\n",
    "    git init\n",
    "    ```\n",
    "\n",
    "4. **Create Initial Files and Directory Structure:**\n",
    "    ```bash\n",
    "    mkdir -p data/raw data/processed models notebooks scripts tests\n",
    "    touch params.yaml dvc.yaml README.md requirements.txt setup.py Dockerfile Makefile\n",
    "    touch .github/workflows/ci_cd_pipeline.yml\n",
    "    ```\n",
    "\n",
    "### **Step 2: Create and Populate Files**\n",
    "\n",
    "1. **`requirements.txt`** - Python dependencies:\n",
    "\n",
    "    ```text\n",
    "    pandas\n",
    "    numpy\n",
    "    scikit-learn\n",
    "    dvc\n",
    "    pytest\n",
    "    flask\n",
    "    gunicorn\n",
    "    pyyaml\n",
    "    joblib\n",
    "    ```\n",
    "\n",
    "2. **`params.yaml`** - Configuration parameters:\n",
    "\n",
    "    ```yaml\n",
    "    data_path: data/raw/housing.csv\n",
    "    processed_data_path: data/processed/processed_housing.csv\n",
    "    model_path: models/house_price_model.pkl\n",
    "\n",
    "    train:\n",
    "      test_size: 0.2\n",
    "      random_state: 42\n",
    "      n_estimators: 100\n",
    "      max_depth: 5\n",
    "    ```\n",
    "\n",
    "3. **Python Scripts in `scripts/`:**\n",
    "\n",
    "    - **`data_preprocessing.py`:**\n",
    "\n",
    "        ```python\n",
    "        import pandas as pd\n",
    "        import yaml\n",
    "\n",
    "        def load_data(config_path):\n",
    "            with open(config_path, 'r') as file:\n",
    "                config = yaml.safe_load(file)\n",
    "            \n",
    "            data = pd.read_csv(config['data_path'])\n",
    "            return data\n",
    "\n",
    "        def preprocess_data(data):\n",
    "            data.fillna(data.mean(), inplace=True)\n",
    "            return data\n",
    "\n",
    "        if __name__ == \"__main__\":\n",
    "            config_path = 'params.yaml'\n",
    "            data = load_data(config_path)\n",
    "            processed_data = preprocess_data(data)\n",
    "            processed_data.to_csv('data/processed/processed_housing.csv', index=False)\n",
    "        ```\n",
    "\n",
    "    - **`train_model.py`:**\n",
    "\n",
    "        ```python\n",
    "        import pandas as pd\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        from sklearn.ensemble import RandomForestRegressor\n",
    "        import joblib\n",
    "        import yaml\n",
    "\n",
    "        def train_model(config_path):\n",
    "            with open(config_path, 'r') as file:\n",
    "                config = yaml.safe_load(file)\n",
    "            \n",
    "            data = pd.read_csv(config['processed_data_path'])\n",
    "            X = data.drop('price', axis=1)\n",
    "            y = data['price']\n",
    "\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X, y, test_size=config['train']['test_size'], random_state=config['train']['random_state']\n",
    "            )\n",
    "\n",
    "            model = RandomForestRegressor(\n",
    "                n_estimators=config['train']['n_estimators'], max_depth=config['train']['max_depth']\n",
    "            )\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            joblib.dump(model, config['model_path'])\n",
    "\n",
    "        if __name__ == \"__main__\":\n",
    "            train_model('params.yaml')\n",
    "        ```\n",
    "\n",
    "    - **`evaluate_model.py`:**\n",
    "\n",
    "        ```python\n",
    "        import pandas as pd\n",
    "        from sklearn.metrics import mean_squared_error\n",
    "        import joblib\n",
    "        import yaml\n",
    "\n",
    "        def evaluate_model(config_path):\n",
    "            with open(config_path, 'r') as file:\n",
    "                config = yaml.safe_load(file)\n",
    "            \n",
    "            model = joblib.load(config['model_path'])\n",
    "            data = pd.read_csv(config['processed_data_path'])\n",
    "            X = data.drop('price', axis=1)\n",
    "            y = data['price']\n",
    "\n",
    "            predictions = model.predict(X)\n",
    "            mse = mean_squared_error(y, predictions)\n",
    "            \n",
    "            print(f\"Model Evaluation: Mean Squared Error = {mse}\")\n",
    "\n",
    "        if __name__ == \"__main__\":\n",
    "            evaluate_model('params.yaml')\n",
    "        ```\n",
    "\n",
    "    - **`deploy_model.py`:**\n",
    "\n",
    "        ```python\n",
    "        from flask import Flask, request, jsonify\n",
    "        import joblib\n",
    "        import pandas as pd\n",
    "\n",
    "        app = Flask(__name__)\n",
    "\n",
    "        model = joblib.load('models/house_price_model.pkl')\n",
    "\n",
    "        @app.route('/predict', methods=['POST'])\n",
    "        def predict():\n",
    "            data = request.get_json()\n",
    "            input_data = pd.DataFrame(data, index=[0])\n",
    "            prediction = model.predict(input_data)\n",
    "            return jsonify({'prediction': prediction[0]})\n",
    "\n",
    "        if __name__ == \"__main__\":\n",
    "            app.run(host='0.0.0.0', port=8000)\n",
    "        ```\n",
    "\n",
    "4. **Unit Tests in `tests/`:**\n",
    "\n",
    "    - **`test_data_preprocessing.py`:**\n",
    "\n",
    "        ```python\n",
    "        import pytest\n",
    "        import pandas as pd\n",
    "        from scripts.data_preprocessing import preprocess_data\n",
    "\n",
    "        def test_preprocess_data():\n",
    "            data = pd.DataFrame({\n",
    "                'feature1': [1, 2, 3, None],\n",
    "                'feature2': [4, None, 6, 7]\n",
    "            })\n",
    "            processed_data = preprocess_data(data)\n",
    "            \n",
    "            assert processed_data.isnull().sum().sum() == 0\n",
    "        ```\n",
    "\n",
    "    - **`test_train_model.py`:**\n",
    "\n",
    "        ```python\n",
    "        import pytest\n",
    "        import os\n",
    "        from scripts.train_model import train_model\n",
    "\n",
    "        def test_train_model():\n",
    "            train_model('params.yaml')\n",
    "            assert os.path.exists('models/house_price_model.pkl')\n",
    "        ```\n",
    "\n",
    "### **Step 3: Docker Configuration**\n",
    "\n",
    "1. **`Dockerfile`:**\n",
    "\n",
    "    ```Dockerfile\n",
    "    # Use an official Python runtime as a parent image\n",
    "    FROM python:3.8-slim\n",
    "\n",
    "    # Set the working directory\n",
    "    WORKDIR /app\n",
    "\n",
    "    # Copy the current directory contents into the container\n",
    "    COPY . /app\n",
    "\n",
    "    # Install any needed packages specified in requirements.txt\n",
    "    RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "    # Make port 8000 available to the world outside this container\n",
    "    EXPOSE 8000\n",
    "\n",
    "    # Run the application\n",
    "    CMD [\"gunicorn\", \"--bind\", \"0.0.0.0:8000\", \"scripts.deploy_model:app\"]\n",
    "    ```\n",
    "\n",
    "2. **Build and Run Docker Container:**\n",
    "\n",
    "    ```bash\n",
    "    docker build -t house-price-prediction .\n",
    "    docker run -p 8000:8000 house-price-prediction\n",
    "    ```\n",
    "\n",
    "### **Step 4: DVC Configuration**\n",
    "\n",
    "1. **`dvc.yaml`:**\n",
    "\n",
    "    ```yaml\n",
    "    stages:\n",
    "      preprocess:\n",
    "        cmd: python scripts/data_preprocessing.py\n",
    "        deps:\n",
    "          - scripts/data_preprocessing.py\n",
    "          - params.yaml\n",
    "        outs:\n",
    "          - data/processed/processed_housing.csv\n",
    "\n",
    "      train:\n",
    "        cmd: python scripts/train_model.py\n",
    "        deps:\n",
    "          - scripts/train_model.py\n",
    "          - data/processed/processed_housing.csv\n",
    "          - params.yaml\n",
    "        outs:\n",
    "          - models/house_price_model.pkl\n",
    "\n",
    "      evaluate:\n",
    "        cmd: python scripts/evaluate_model.py\n",
    "        deps:\n",
    "          - scripts/evaluate_model.py\n",
    "          - models/house_price_model.pkl\n",
    "          - data/processed/processed_housing.csv\n",
    "          - params.yaml\n",
    "    ```\n",
    "\n",
    "2. **Initialize and Run DVC:**\n",
    "\n",
    "    ```bash\n",
    "    dvc init\n",
    "    dvc add data/raw/housing.csv\n",
    "    dvc repro\n",
    "    ```\n",
    "\n",
    "### **Step 5: GitHub Actions CI/CD Pipeline**\n",
    "\n",
    "1. **Create a GitHub Repository:**\n",
    "    - Go to GitHub and create a new repository.\n",
    "    - Clone the repository to your local machine.\n",
    "\n",
    "2. **Push the Local\n",
    "\n",
    " Project to GitHub:**\n",
    "\n",
    "    ```bash\n",
    "    git add .\n",
    "    git commit -m \"Initial commit\"\n",
    "    git remote add origin https://github.com/yourusername/HousePricePrediction.git\n",
    "    git push -u origin master\n",
    "    ```\n",
    "\n",
    "3. **GitHub Actions Workflow - `.github/workflows/ci_cd_pipeline.yml`:**\n",
    "\n",
    "    ```yaml\n",
    "    name: CI/CD Pipeline\n",
    "\n",
    "    on:\n",
    "      push:\n",
    "        branches:\n",
    "          - master\n",
    "      pull_request:\n",
    "        branches:\n",
    "          - master\n",
    "\n",
    "    jobs:\n",
    "      build:\n",
    "        runs-on: ubuntu-latest\n",
    "\n",
    "        steps:\n",
    "        - name: Checkout code\n",
    "          uses: actions/checkout@v2\n",
    "\n",
    "        - name: Set up Python\n",
    "          uses: actions/setup-python@v2\n",
    "          with:\n",
    "            python-version: '3.8'\n",
    "\n",
    "        - name: Install dependencies\n",
    "          run: |\n",
    "            python -m venv venv\n",
    "            source venv/bin/activate\n",
    "            pip install --upgrade pip\n",
    "            pip install -r requirements.txt\n",
    "\n",
    "        - name: Run tests\n",
    "          run: |\n",
    "            source venv/bin/activate\n",
    "            pytest tests/\n",
    "\n",
    "        - name: Set up Docker Buildx\n",
    "          uses: docker/setup-buildx-action@v1\n",
    "\n",
    "        - name: Log in to DockerHub\n",
    "          uses: docker/login-action@v1\n",
    "          with:\n",
    "            username: ${{ secrets.DOCKER_USERNAME }}\n",
    "            password: ${{ secrets.DOCKER_PASSWORD }}\n",
    "\n",
    "        - name: Build and push Docker image\n",
    "          run: |\n",
    "            docker build . -t ${{ secrets.DOCKER_USERNAME }}/house-price-prediction\n",
    "            docker push ${{ secrets.DOCKER_USERNAME }}/house-price-prediction\n",
    "\n",
    "        - name: Deploy to server\n",
    "          run: |\n",
    "            ssh user@server_ip 'docker pull ${{ secrets.DOCKER_USERNAME }}/house-price-prediction && docker run -d -p 8000:8000 ${{ secrets.DOCKER_USERNAME }}/house-price-prediction'\n",
    "    ```\n",
    "\n",
    "4. **Set Up GitHub Secrets:**\n",
    "    - Navigate to your GitHub repository settings and add secrets for Docker credentials and server information.\n",
    "    - Ensure the secrets match those referenced in your GitHub Actions workflow file.\n",
    "\n",
    "5. **Test the Workflow**\n",
    "    - Push changes to the `master` branch and verify that the CI/CD pipeline runs successfully on GitHub.\n",
    "\n",
    "### **Step 6: Final Steps**\n",
    "\n",
    "1. **README.md:**\n",
    "\n",
    "    ```markdown\n",
    "    # House Price Prediction\n",
    "\n",
    "    This project is a machine learning pipeline to predict house prices. The project uses a CI/CD pipeline for continuous integration and continuous deployment with Docker and GitHub Actions.\n",
    "\n",
    "    ## Project Structure\n",
    "\n",
    "    - **data/**: Contains raw and processed data.\n",
    "    - **models/**: Contains trained models.\n",
    "    - **scripts/**: Python scripts for data preprocessing, model training, evaluation, and deployment.\n",
    "    - **tests/**: Unit tests for the scripts.\n",
    "    - **.github/**: GitHub Actions CI/CD pipeline configuration.\n",
    "    - **Dockerfile**: Docker configuration for the project.\n",
    "    - **Makefile**: Makefile for common commands.\n",
    "    - **dvc.yaml**: DVC pipeline configuration.\n",
    "    - **params.yaml**: Hyperparameters and configuration settings.\n",
    "\n",
    "    ## Setup\n",
    "\n",
    "    1. Clone the repository.\n",
    "    2. Create a virtual environment: `python3 -m venv venv`\n",
    "    3. Activate the environment: `source venv/bin/activate`\n",
    "    4. Install dependencies: `pip install -r requirements.txt`\n",
    "    5. Run the pipeline: `dvc repro`\n",
    "\n",
    "    ## CI/CD Pipeline\n",
    "\n",
    "    The CI/CD pipeline is set up using GitHub Actions. It runs the tests, builds a Docker image, and deploys it to a server.\n",
    "\n",
    "    ## Usage\n",
    "\n",
    "    - Preprocess data: `python scripts/data_preprocessing.py`\n",
    "    - Train the model: `python scripts/train_model.py`\n",
    "    - Evaluate the model: `python scripts/evaluate_model.py`\n",
    "    ```\n",
    "\n",
    "2. **Makefile:**\n",
    "\n",
    "    ```Makefile\n",
    "    install:\n",
    "        pip install -r requirements.txt\n",
    "\n",
    "    test:\n",
    "        pytest tests/\n",
    "\n",
    "    run:\n",
    "        python scripts/train_model.py\n",
    "\n",
    "    deploy:\n",
    "        docker build -t house-price-prediction .\n",
    "        docker run -p 8000:8000 house-price-prediction\n",
    "    ```\n",
    "\n",
    "### **Conclusion**\n",
    "\n",
    "This setup covers everything from creating the project structure to deploying the ML model using a CI/CD pipeline integrated with GitHub Actions. The pipeline will automatically run tests, build Docker images, and deploy them when changes are pushed to the `master` branch.\n",
    "\n",
    "By following these steps, you will successfully set up a fully functional CI/CD pipeline for your machine learning project, ensuring continuous integration and deployment in a professional and automated manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7406231-6dfb-4cd1-94df-3ea57f15628f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a00552f-d028-4f35-b29c-7c3be1d63903",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990b0695-439b-423c-9fce-46fb35529f05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8089f374-bc06-4310-bdae-ca669f489b2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa0e10c-88bc-4723-aa6a-b2f00777f361",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1619721d-0496-43ad-830e-64617f8de9c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee49582-5bc1-429f-bc0f-76286c43718c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a96e85-f806-4830-8ebe-7981d7a8f328",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0937f759-7f67-47b7-a450-7a48416d5ed0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d52359-fc21-424d-81db-cd1a058d2e48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65d1b93-520f-45b3-9b19-7f7a96cb3585",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86887d9-98b2-4e6c-93dd-c695a156c627",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c887c7c-1a83-4c59-9ddf-b41284386272",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791c7ab6-ced0-4119-896c-acc9fa7269fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f776234-d1db-45ec-8d23-d00a4af5f43c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccd6fea-9298-4ba6-a6f8-8a9b600bbca4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff8acb5-02b5-4129-8b40-6327d6d9d0b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c17a3b-cd33-45e4-9d8a-b2b94855217a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7a1aa1-2d6e-4c04-a551-b0e76dfed48e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f52be9-0279-4519-9fcd-a0753363e653",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
